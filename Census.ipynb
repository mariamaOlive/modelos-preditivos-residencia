{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final - Modelos Preditivos - Dataset do Censo\n",
    "\n",
    "## Grupo:\n",
    "- Lucas Natan Correia Couri - lncc2@cin.ufpe.br\n",
    "- Mariama Celi Serafim de Oliveira - mcso@cin.ufpe.br\n",
    "- Laianna Lana Virginio da Silva - llvs2@cin.ufpe.br\n",
    "- Priscilla Amarante de Lima - pal4@cin.ufpe.br\n",
    "- Liviany Reis Rodrigues - lrr@cin.ufpe.br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from numpy.random import randn\n",
    "from numpy.random import seed\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determinação da SEED utilizada no projeto\n",
    "SEED = 6138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Entendimento do Negócio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do problema é determinar se uma pessoa ganha mais ou menos de 50 mil dólares. Serão utilizadas as informações do censo americano (14 features) a fim de gerar os modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compreensão dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Descrição da base: https://archive.ics.uci.edu/ml/datasets/census+income\n",
    "\n",
    "Número de Instâncias:\n",
    "* Dados de Treino: 32561\n",
    "* Dados de Teste: 16281\n",
    "\n",
    "Valores Ausentes:\n",
    "* Foram substituídos por \" ?\"\n",
    "\n",
    "Número de Atributos: 14\n",
    "* age: continuous.\n",
    "* workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "* fnlwgt: continuous.\n",
    "* education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "* education-num: continuous.\n",
    "* marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "* occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "* relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "* race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "* sex: Female, Male.\n",
    "* capital-gain: continuous.\n",
    "* capital-loss: continuous.\n",
    "* hours-per-week: continuous.\n",
    "* native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "Distribuição da Classe:\n",
    "* '>50K' , '<=50K'.\n",
    "    *   '>50K'  : 23.93% / 24.78% (without ' ?')\n",
    "    *   '<=50K' : 76.07% / 75.22% (without ' ?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando a Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
    "df = pd.read_csv(\"Dados/adult.data\", names = columns_name, index_col = False)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta seção, serão analisados a distribuição e característica dos atributos, valores faltantes, possíveis outliers e nível de separatibilidade. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mudando os atributos para seus tipos corretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['workclass'] = df['workclass'].astype('category')\n",
    "df['education'] = df['education'].astype('category')\n",
    "df['marital-status'] = df['marital-status'].astype('category')\n",
    "df['occupation'] = df['occupation'].astype('category')\n",
    "df['relationship'] = df['relationship'].astype('category')\n",
    "df['race'] = df['race'].astype('category')\n",
    "df['sex'] = df['sex'].astype('category')\n",
    "df['native-country'] = df['native-country'].astype('category')\n",
    "df['class'] = df['class'].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descrição dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumário dos dados contínuos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados Duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checando dados duplicados. Ao final, verificamos que não há linhas duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preenchendo Dados Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamento_faltantes(df, columns_name):\n",
    "    ## Printa os atributos com dados faltantes (\" ?\")\n",
    "    for coluna in columns_name:\n",
    "        if len(df[df[coluna] == \" ?\"]) > 0:\n",
    "            print(coluna)\n",
    "            print(len(df[df[coluna] == \" ?\"]))\n",
    "    \n",
    "    ## Tratamento dos dados faltantes:\n",
    "    atr_faltantes = [\"workclass\", \"occupation\", \"native-country\"]\n",
    "    for atr in atr_faltantes:\n",
    "        categorias_atr = df.groupby(atr).sum().index.tolist()\n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        label_encoder.fit(categorias_atr)\n",
    "        df[atr] = df[atr].replace(\" ?\", np.nan)\n",
    "        df[atr] = df[atr].interpolate(method = 'pad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamento dos dados faltantes: \n",
    "1. Transforma para numérico (LabelEnconder) \n",
    "2. Substitui \" ?\" por NaN (replace)\n",
    "3. Utiliza a frequência dos vizinhos mais próximos para estimar um valor para NaN (interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tratamento_faltantes(df, columns_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checando Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hours-per-week'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hours-per-week'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['capital-gain'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['capital-gain'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['capital-loss'].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['capital-loss'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colunas Redundantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"education\" e \"education-num\" significam a mesma coisa. Vamos utilizar \"education-num\" e dropar \"education\", já que \"education-num\" é a codificação ordinal de \"education\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education-num'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequência das Variáveis Categóricas (Value_Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['workclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['marital-status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['relationship'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['native-country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No dataset de treino há apenas uma observação como \" Holand-Netherlands\", diante do tamanho do dataset (mais de 30mil linhas) optou-se por remover essa única linha com native-country=\" Holand-Netherlands\" de forma a evitar problemas de ausência do valor no dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['native-country'] != \" Holand-Netherlands\"]['native-country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e Processando o Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"Dados/adult.test\", names = columns_name, index_col = False, skiprows = 1)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preenchendo Dados Faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando o mesmo procedimento de input utilizado no conjunto de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tratamento_faltantes(df_test, columns_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificação das Variáveis Categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos a codificação One Hot Encoding, dado que os modelos a serem utilizados necessitam de entradas numérica. Esse tipo de codifição é indicado para variáveis categóricas nominais, pois atribui distâncias uniformes às categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoder(df):\n",
    "\n",
    "    colunas_cat = [\"workclass\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "    \n",
    "    for coluna in colunas_cat:\n",
    "\n",
    "        #print(coluna)\n",
    "        df_coluna = pd.get_dummies(df[coluna], prefix=coluna)\n",
    "        df = df.join(df_coluna)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = onehot_encoder(df)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = onehot_encoder(df_test)\n",
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que no conjunto de teste não havia observações com o valor 'Holand-Netherlands' em 'native-country', fato que causava inconsistência com o conjunto de treinamento ao realizar o Hot Enconding. A fim de resolver esse problema, adicionamos manualmente a coluna 'native-country_ Holand-Netherlands'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coluna = 'native-country_ Holand-Netherlands'\n",
    "#df[coluna]\n",
    "df_test[coluna] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizando Variáveis Contínuas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indica-se normalização uma vez que alguns modelos que serão utilizados são baseados em distância. Caso haja escala de distância muito distinta, pode ocorrer enviesamento em algumas features e consequentemente comprometimento na performance do classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = [\"age\", \"fnlwgt\", \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"education-num\"]\n",
    "\n",
    "df[colunas] = normalize.fit_transform(df[colunas])\n",
    "df_test[colunas] = normalize.fit_transform(df_test[colunas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[colunas].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo o Conjuntos de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_drop = [\"class\", \"education\", \"workclass\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "\n",
    "X_train = df.drop(colunas_drop, axis = 1).to_numpy()\n",
    "y_train = df[\"class\"].values\n",
    "X_test = df_test.drop(colunas_drop, axis = 1).to_numpy()\n",
    "y_test = df_test[\"class\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificando as classes para valores numéricos:\n",
    "- '<=50K' :  0\n",
    "- '>50K'  : 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot do conjunto de Treino (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "X_pca = pca.fit_transform(preprocessing.minmax_scale(X_train))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train)\n",
    "\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower left\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo tamanho do folder no cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = model_selection.StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliação do KNN foram considerados o número de vizinhos (n_neighbors) e o tipo de distância (metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_knn(X_train, y_train, parameters, cv, SEED):\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    \n",
    "    search = RandomizedSearchCV(knn,\n",
    "                                parameters,\n",
    "                                n_iter = 50,\n",
    "                                scoring = 'accuracy',\n",
    "                                n_jobs = -1,\n",
    "                                cv = cv,\n",
    "                                random_state = SEED)\n",
    "    \n",
    "    result_knn = search.fit(X_train, y_train)\n",
    "    \n",
    "    print('=========Resultados do Random Search para o KNN==========')\n",
    "    print(f'Melhor Score: {result_knn.best_score_}')\n",
    "    print(f'Melhores Hiperparâmetros: {result_knn.best_params_}')\n",
    "    \n",
    "    return result_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo parâmetros para avaliação\n",
    "parameters = dict()\n",
    "parameters['n_neighbors'] = range(1, 100)\n",
    "parameters['metric'] = ['euclidean','manhattan']\n",
    "\n",
    "# result_knn = val_knn(X_train, y_train, parameters, cv, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhores parâmetros para KNN:**\n",
    "- n_neighbors: 23\n",
    "- metric': 'manhattan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árvore de Decisão Simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliação da árvore de decisão foram considerados a profundidade da árvore (max_depth), o critério de divisão (criteon), número mínimo de amostra para realizar divisão (min_samples_split), mínimo de amostras necessárias para ser uma folha (min_samples_leaf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_tree(X_train, y_train, parameters, cv, SEED):\n",
    "    \n",
    "    decisionTree = DecisionTreeClassifier()\n",
    "    \n",
    "    search = RandomizedSearchCV(decisionTree,\n",
    "                                parameters,\n",
    "                                n_iter = 50,\n",
    "                                scoring = 'accuracy',\n",
    "                                n_jobs = -1,\n",
    "                                cv = cv,\n",
    "                                random_state = SEED)\n",
    "    \n",
    "    result_tree = search.fit(X_train, y_train)\n",
    "    \n",
    "    print('=========Resultados do Random Search para a Árvore de Decisão Simples==========')\n",
    "    print(f'Melhor Score: {result_tree.best_score_}')\n",
    "    print(f'Melhores Hiperparâmetros: {result_tree.best_params_}')\n",
    "    \n",
    "    return result_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo parâmetros para avaliação\n",
    "parameters = dict()\n",
    "parameters['criterion'] = ['gini', 'entropy']\n",
    "parameters['max_depth'] = range(1,30)\n",
    "parameters['min_samples_split'] = range(1,20)\n",
    "parameters['min_samples_leaf'] = range(1,10)\n",
    "\n",
    "# result_tree = val_tree(X_train, y_train, parameters, cv, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhores parâmetros para Árvore de Decisão:**\n",
    "\n",
    "- criterion: 'gini'\n",
    "- max_depth: 11\n",
    "- min_samples_split: 15\n",
    "- min_samples_leaf: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliação do Random Forest foram considerados o número de árvores da floresta (n_estimators), o critério de divisão (criteon), número mínimo de amostra para realizar divisão (min_samples_split), mínimo de amostras necessárias para ser uma folha (min_samples_leaf) e número máximo de features (max_features) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_rf(X_train, y_train, parameters, cv, SEED):\n",
    "\n",
    "    rf = RandomForestClassifier(random_state = SEED)\n",
    "\n",
    "    search = GridSearchCV(rf,\n",
    "                          parameters,\n",
    "                          scoring = \"accuracy\",\n",
    "                          n_jobs = -1,\n",
    "                          cv = cv)\n",
    "\n",
    "    result_rf = search.fit(X_train, y_train)\n",
    "    \n",
    "    print('=========Resultados do Grid Search para Random Forest==========')\n",
    "    print(f'Melhor Score: {result_rf.best_score_}')\n",
    "    print(f'Melhores Hiperparâmetros: {result_rf.best_params_}')\n",
    "\n",
    "    return result_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 1 (18min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(10, 301, 20)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "\n",
    "#result_rf_1 = val_rf(X_train, y_train, parameters, cv, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 2 (47min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(300, 451, 10)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "\n",
    "#result_rf_2 = search_rf(parameters, cv, X_train, y_train, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 3 (17min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters['n_estimators'] = range(408, 413, 1)\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "\n",
    "#result_rf_3 = search_rf(parameters, cv, X_train, y_train, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters['n_estimators'] = [190, 440, 412]\n",
    "parameters['criterion'] = [\"gini\", \"entropy\"]\n",
    "parameters['max_features'] = [\"auto\", \"sqrt\", \"log2\"]\n",
    "parameters['min_samples_leaf'] = [1, 4]\n",
    "parameters['min_samples_split'] = [2, 10]\n",
    "#parameters['max_depth'] = [10, 100, None]#[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "\n",
    "#result_rf_4 = search_rf(parameters, cv, X_train, y_train, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhores parâmetros para Random Forest:**\n",
    "\n",
    "- n_estimators: 440\n",
    "- criterion: \"gini\"\n",
    "- max_features: \"auto\"\n",
    "- min_samples_split: 4\n",
    "- min_samples_leaf: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliação da Rede Neural MLP foram considerados números de camadas escondidas (hidden_layer_sizes), a função de ativação (activation), algorítmos de treinamento (solver), tipo de taxa de aprendizado (learning_rate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_mlp(X_train, y_train, parameters, cv, SEED, search_type = 0):\n",
    "\n",
    "    mlp = MLPClassifier(random_state = SEED)\n",
    "    \n",
    "    if search_type == 0:\n",
    "        search = GridSearchCV(mlp,\n",
    "                            parameters,\n",
    "                            scoring = \"accuracy\",\n",
    "                            n_jobs = -1,\n",
    "                            cv = cv)\n",
    "    else:\n",
    "        search = RandomizedSearchCV(mlp,\n",
    "                                    parameters,\n",
    "                                    n_iter = 50,\n",
    "                                    scoring = \"accuracy\",\n",
    "                                    n_jobs = -1,                            \n",
    "                                    cv = cv,\n",
    "                                    random_state = SEED)\n",
    "\n",
    "    result_mlp = search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'=========Resultados do {\"Grid\" if (search_type == 0) else \"Random\"} Search para MLP==========')\n",
    "    print(f'Melhor Score: {result_mlp.best_score_}')\n",
    "    print(f'Melhores Hiperparâmetros: {result_mlp.best_params_}')\n",
    "\n",
    "    return result_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 1 (55 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo parâmetros do MPL\n",
    "parameters = dict()\n",
    "parameters[\"hidden_layer_sizes\"] = [(4,4), (20,15),(50,50), (100,50)] #Testando com 2 camadas\n",
    "parameters[\"activation\"] = [\"logistic\", \"tanh\", \"relu\", \"identity\"]\n",
    "parameters[\"solver\"] = [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "parameters[\"learning_rate\"] = [\"constant\", \"invscaling\", \"adaptive\"] \n",
    "\n",
    "# result_mlp_1 = val_mlp(X_train, y_train, parameters, cv, SEED, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionando leaning_rate e retirando algumas opções\n",
    "parameters = dict()\n",
    "parameters[\"hidden_layer_sizes\"] = [(4,10,4),(20,10,5),(250, 100, 50)] #Testando com 3 camadas \n",
    "parameters[\"activation\"] = [\"logistic\", \"tanh\", \"relu\", \"identity\"]\n",
    "parameters[\"solver\"] =  [\"lbfgs\", \"sgd\", \"adam\"]\n",
    "parameters[\"learning_rate\"] = [\"constant\", \"invscaling\", \"adaptive\"] \n",
    "\n",
    "#result_mlp_2 = val_mlp(X_train, y_train, parameters, cv, SEED, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testando melhores resultados da tentativa 1 e 2\n",
    "parameters = dict()\n",
    "parameters[\"hidden_layer_sizes\"] = [(20,15), (250, 100, 50)]\n",
    "parameters[\"activation\"] = [\"tanh\", \"logistic\"]\n",
    "parameters[\"solver\"] = [\"lbfgs\"]\n",
    "parameters[\"learning_rate\"] = [\"constant\", \"adaptive\"] \n",
    "\n",
    "#result_mlp_3 = val_mlp(X_train, y_train, parameters, cv, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testando camadas mais profundas\n",
    "parameters = dict()\n",
    "parameters[\"hidden_layer_sizes\"] = [(5,5,5,5), (10,10,10,10), (50,50,50,50), (10,10,10,10,10)]\n",
    "parameters[\"activation\"] = [\"tanh\"]\n",
    "parameters[\"solver\"] = [\"lbfgs\"]\n",
    "parameters[\"learning_rate\"] = [\"constant\"] \n",
    "\n",
    "#result_mlp_4 = val_mlp(X_train, y_train, parameters, cv, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhores parâmetros para Rede Neural MLP:**\n",
    "\n",
    "- hidden_layer_sizes: (50, 50, 50, 50)\n",
    "- activation: 'tanh'\n",
    "- solver: 'lbfgs'\n",
    "- learning_rate: 'constant'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comitê de Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliação do Comitê de Redes Neurais foram considerados números de camadas escondidas (hidden_layer_sizes), número de interações (max_iter), quantidade de classificadores no bagging (n_estimators), tamanho da amostra escolhida para treinar os classificadores (max_samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_bgc(X_train, y_train, parameters, cv, SEED, hidden_l, max_iterations):\n",
    "       \n",
    "    mlp = MLPClassifier(hidden_layer_sizes = hidden_l,\n",
    "                        max_iter = max_iterations,\n",
    "                        random_state = SEED\n",
    "                        )\n",
    "    \n",
    "    search = GridSearchCV(BaggingClassifier(mlp),\n",
    "                          parameters,\n",
    "                          scoring = \"accuracy\",\n",
    "                          #cv = cv,\n",
    "                          n_jobs= -1\n",
    "                          )\n",
    "\n",
    "    result_bgc = search.fit(X_train, y_train)\n",
    "   \n",
    "    print(f'=========Resultados do Grid Search para Bagging Classifier com Redes Neurais MLP==========')\n",
    "    print(f'Melhor Score: {result_bgc.best_score_}')\n",
    "    print(f'Melhores Hiperparâmetros: {result_bgc.best_params_}')\n",
    "\n",
    "    return result_bgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters[\"n_estimators\"] = [10, 20, 30]\n",
    "parameters[\"max_samples\"] = [0.1, 0.2]\n",
    "\n",
    "hidden_l = (10, 10)\n",
    "max_iterations = 200\n",
    "\n",
    "#result_bgc = val_bgc(X_train, y_train, parameters, cv, SEED, hidden_l, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhores parâmetros para Comitê de Redes Neurais:**\n",
    "\n",
    "- hidden_layer_sizes: (10, 10)\n",
    "- max_iter: 200\n",
    "- n_estimators: 30\n",
    "- max_samples: 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comitê Heterogêneo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este comitê utiliza quatro estimadores fracos (KNN, árvore de decisão, random forest e rede neural MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_ensemble(X_train, y_train, estimators, cv):\n",
    "       \n",
    "    ensemble = VotingClassifier(estimators)\n",
    "    \n",
    "    result_ens = model_selection.cross_val_score(ensemble, X_train, y_train, cv = cv)\n",
    "   \n",
    "    print(f'=========Resultados do Comitê Heterogêneo com KNN, Árvore de Decisão, Random Forest e MLP==========')\n",
    "    print(f'Score: {result_ens.mean()}')\n",
    "\n",
    "    return result_ens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators1 = []\n",
    "\n",
    "model1 = KNeighborsClassifier()\n",
    "estimators1.append(('KNN', model1))\n",
    "\n",
    "model2 = DecisionTreeClassifier(max_depth = 7)\n",
    "estimators1.append(('Tree', model2))\n",
    "\n",
    "model3 = RandomForestClassifier()\n",
    "estimators1.append(('RF', model3))\n",
    "\n",
    "model4 = MLPClassifier(hidden_layer_sizes = (10,10))\n",
    "estimators1.append(('MLP', model4))\n",
    "\n",
    "#result_ens1 = val_ensemble(X_train, y_train, estimators1, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators2 = []\n",
    "\n",
    "model1 = KNeighborsClassifier()\n",
    "estimators2.append(('KNN', model1))\n",
    "\n",
    "model2 = MLPClassifier(hidden_layer_sizes = (10,10))\n",
    "estimators2.append(('MLP', model2))\n",
    "\n",
    "#result_ens2 = val_ensemble(X_train, y_train, estimators2, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators3 = []\n",
    "\n",
    "model1 = DecisionTreeClassifier(max_depth = 6)\n",
    "estimators3.append(('Tree', model1))\n",
    "\n",
    "model2 = RandomForestClassifier(n_estimators = 50)\n",
    "estimators3.append(('RF', model2))\n",
    "\n",
    "model3 = DecisionTreeClassifier(max_depth = 8)\n",
    "estimators3.append(('Tree', model3))\n",
    "\n",
    "model4 = RandomForestClassifier()\n",
    "estimators3.append(('RF', model4))\n",
    "\n",
    "\n",
    "#result_ens3 = val_ensemble(X_train, y_train, estimators3, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melhores Modelos Encontrados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição dos modelos de classificação com as melhores configurações encontradas na seção anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criação dos modelos com os melhores parâmetros\n",
    "knn = KNeighborsClassifier(n_neighbors = 23, metric = 'manhattan')\n",
    "\n",
    "arvore = DecisionTreeClassifier(min_samples_split = 15, min_samples_leaf = 2, max_depth = 11, criterion = 'gini')\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 440,\n",
    "                            criterion = \"gini\",\n",
    "                            max_features = \"auto\",\n",
    "                            min_samples_leaf = 4,\n",
    "                            min_samples_split = 10,\n",
    "                            random_state = SEED)\n",
    "\n",
    "mlp = MLPClassifier(activation = 'tanh',\n",
    "                    hidden_layer_sizes = (50, 50, 50, 50),\n",
    "                    learning_rate = 'constant',\n",
    "                    solver = 'lbfgs')\n",
    "\n",
    "comite_mlp = BaggingClassifier(MLPClassifier(hidden_layer_sizes = (10, 10),\n",
    "                                                              max_iter = 10,\n",
    "                                                              random_state = SEED),\n",
    "                               n_estimators = 10,\n",
    "                               max_samples = 10\n",
    "                              )\n",
    "\n",
    "comite_h = VotingClassifier(estimators1) #Falta encontrar o melhor estimador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionamos os modelos em uma lista a fim de coletar suas avaliações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('knn', knn))\n",
    "models.append(('arvore', arvore))\n",
    "models.append(('random', rf))\n",
    "models.append(('mlp', mlp))\n",
    "models.append(('comite_mlp', comite_mlp))\n",
    "models.append(('comite_h', comite_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução do Modelo Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliação de cada modelo nas amotragens estratificas\n",
    "print('\\nDesempenhos médios dos modelos:')\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    \n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv = cv, scoring = 'accuracy')\n",
    "    \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "    print(f\"{name}: {cv_results.mean()} ({cv_results.std()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos valores encontrados na validação cruzada acima, iremos realizar o teste de significânica estatística a fim de comparar os classificadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos o teste **Kruskal-Wallis**\n",
    "\n",
    "Passos: \n",
    "1. Detectamos se havia distribuições distintas.\n",
    "2. Caso encontrado uma distribuição distinta, realizamos o teste Kruskal par a par para determinar qual classificador é estatísticamente diferente do outro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste de Hipótese Analisando o p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.kruskal(results[0], results[1], results[2], results[3], results[4], results[5])\n",
    "print(f\"p_value: {p}. Comparison stats: {stat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('\\nMesma distribuição (aceita H0)')\n",
    "else:\n",
    "    print('\\nDiferentes distribuições (rejeita H0)')\n",
    "\n",
    "alg = [\"KNN        \", \"ÁRVORE     \", \"RF         \", \"MLP        \", \"COMITÊ MLP \", \"COMITÊ HET.\"]\n",
    "print(\"\\nCOMPARAÇÃO:\")\n",
    "for i in range(len(results)):\n",
    "    for j in range(i+1, len(results)):\n",
    "        print(f'   {alg[i]} | {alg[j]} -> statistic = {round(stats.kruskal(results[i],results[j]).statistic, 3)},\\tp_value = {round(stats.kruskal(results[i],results[j]).pvalue, 4)},\\t{\"Mesma distribuição (aceita H0)\" if (stats.kruskal(results[i],results[j]).pvalue > alpha) else \"Diferentes distribuições (rejeita H0)\"} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando o Modelo na Base de Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento dos modelos no conjunto de treino completo (sem divisão de validação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "arvore.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "mlp.fit(X_train, y_train)\n",
    "comite_mlp.fit(X_train, y_train)\n",
    "comite_h.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predição de cada modelo para a base de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prediction_knn = knn.predict(X_test)\n",
    "y_test_prediction_arvore = arvore.predict(X_test)\n",
    "y_test_prediction_rf = rf.predict(X_test)\n",
    "y_test_prediction_mlp = mlp.predict(X_test)\n",
    "y_test_prediction_comite_mlp = comite_mlp.predict(X_test)\n",
    "y_test_prediction_comite_h = comite_h.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apresentação de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAcurácia KNN: Treinamento\",  knn.score(X_train, y_train),\" Teste\" ,knn.score(X_test, y_test))\n",
    "print(\"Clasification Report:\", classification_report(y_test, y_test_prediction_knn))\n",
    "print(\"Confussion Matrix:\\n\", confusion_matrix(y_test, y_test_prediction_knn))\n",
    "\n",
    "print(\"\\nAcurácia ÁRVORE: Treinamento\",  arvore.score(X_train, y_train),\" Teste\" , arvore.score(X_test, y_test))\n",
    "print(\"Clasification Report:\", classification_report(y_test, y_test_prediction_arvore))\n",
    "print(\"Confussion Matrix:\\n\", confusion_matrix(y_test, y_test_prediction_arvore))\n",
    "\n",
    "print(\"\\nAcurácia RF: Treinamento\",  rf.score(X_train, y_train),\" Teste\" ,rf.score(X_test, y_test))\n",
    "print(\"Clasification Report:\", classification_report(y_test, y_test_prediction_rf))\n",
    "print(\"Confussion Matrix:\\n\", confusion_matrix(y_test, y_test_prediction_rf))\n",
    "\n",
    "print(\"\\nAcurácia MLP: Treinamento\",  mlp.score(X_train, y_train),\" Teste\" ,mlp.score(X_test, y_test))\n",
    "print(\"Clasification Report:\", classification_report(y_test, y_test_prediction_mlp))\n",
    "print(\"Confussion Matrix:\\n\", confusion_matrix(y_test, y_test_prediction_mlp))\n",
    "\n",
    "print(\"\\nAcurácia COMITÊ MLP: Treinamento\",  comite_mlp.score(X_train, y_train),\" Teste\" ,comite_mlp.score(X_test, y_test))\n",
    "print(\"Clasification Report:\", classification_report(y_test, y_test_prediction_comite_mlp))\n",
    "print(\"Confussion Matrix:\\n\", confusion_matrix(y_test, y_test_prediction_comite_mlp))\n",
    "\n",
    "print(\"\\nAcurácia COMITÊ HETEROGÊNEO: Treinamento\",  comite_h.score(X_train, y_train),\" Teste\" ,comite_h.score(X_test, y_test))\n",
    "print(\"Clasification Report:\", classification_report(y_test, y_test_prediction_comite_h))\n",
    "print(\"Confussion Matrix:\\n\", confusion_matrix(y_test, y_test_prediction_comite_h))\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colunas_drop = [\"class\", \"education\", \"workclass\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = arvore.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation1 = tree.export_text(arvore)\n",
    "print(text_representation1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotar a melhor árvore\n",
    "feature_names = df.drop(colunas_drop, axis = 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(feature_names, arvore.feature_importances_), reverse=True,  key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "\n",
    "_ = tree.plot_tree(arvore, \n",
    "                   feature_names = feature_names,  \n",
    "                  #  class_names=list(label_encoder.classes_),\n",
    "                   filled=True,\n",
    "                   max_depth = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propor sugestões de decisões com base nos atributos"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8003219c8c57211ee3be347d121ba14ebad7276cdae3d94be72d9e4e17f9edd5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
